{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70. 単語ベクトルの和による特徴量Permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# データの読込\n",
    "df=pd.read_csv('NewsAggregatorDataset/newsCorpora.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "#df[df['PUBLISHER']  == 'Reuters']\n",
    "df = df[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])]\n",
    "df = df[['TITLE','CATEGORY']]\n",
    "\n",
    "#分割はlist、ndarray、DataFrame\n",
    "\n",
    "train,valid_once=train_test_split(df,test_size=0.2,shuffle=True,random_state=100,stratify=df['CATEGORY'])\n",
    "valid,test=train_test_split(valid_once,test_size=0.5,shuffle=True,random_state=100,stratify=valid_once['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('~/Desktop/ニューラル勉強会/hlab2023-nlp100/7/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0200,  0.1468, -0.1442,  ..., -0.0660,  0.0160,  0.1448],\n",
      "        [-0.0325,  0.0205,  0.0668,  ..., -0.0393,  0.0037, -0.0476],\n",
      "        [ 0.0546, -0.0825, -0.0760,  ..., -0.0602,  0.0048, -0.0069],\n",
      "        ...,\n",
      "        [-0.0367,  0.0171, -0.0423,  ..., -0.0434,  0.0469,  0.0620],\n",
      "        [ 0.0225,  0.0613,  0.0038,  ..., -0.0829, -0.0349, -0.0185],\n",
      "        [-0.0289,  0.0737, -0.0573,  ..., -0.0636,  0.0144,  0.1008]])\n",
      "tensor([2, 0, 1,  ..., 0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "def processing(data):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    label={'b':0,'t':1,'e':2,'m':3}\n",
    "    for title,categoly in data:\n",
    "        table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        title = title.translate(table)  # 記号をスペースに置換\n",
    "        title = re.sub('[0-9]+','0',title)\n",
    "        title = title.lower()\n",
    "        title = title.split()\n",
    "        vec = [model[word] for word in title if word in model]\n",
    "        x.append(sum(vec) / len(vec))\n",
    "        y.append(label[categoly])\n",
    "    return torch.tensor(x),torch.tensor(y)\n",
    "\n",
    "train = np.array(train)\n",
    "valid = np.array(valid)\n",
    "test  = np.array(test)\n",
    "\n",
    "x_train, y_train = processing(train)\n",
    "x_valid, y_valid = processing(valid)\n",
    "x_test , y_test  = processing(test)\n",
    "\n",
    "torch.Size(x_train.size())\n",
    "print(x_train)\n",
    "print(y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLPNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "        #nn.init.normal_(self.fc.weight, 0.0, 1.0)   #正規乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0368,  1.7777,  0.8214,  0.5145]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0347, 0.5790, 0.2225, 0.1637]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0347, 0.5790, 0.2225, 0.1637],\n",
      "        [0.1497, 0.4886, 0.2236, 0.1380],\n",
      "        [0.0493, 0.3584, 0.3407, 0.2516],\n",
      "        [0.1980, 0.2933, 0.1872, 0.3215]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "#model = SLPNet(300, 4)\n",
    "#わざわざSLNetを作る意味とは\n",
    "model = nn.Linear(300, 4)\n",
    "nn.init.normal_(model.weight, 0.0, 1.0)   #正規乱数で重みを初期化\n",
    "\n",
    "print(model(x_train[:1]))\n",
    "#そもそもマイナスに対してどうやってsoftmaxしてるのか不明\n",
    "\n",
    "#model()でも呼び出されるのはforward\n",
    "#なるべくforwardを直接使用するのは避ける\n",
    "y_hat_1 = torch.softmax(model(x_train[:1]), dim=-1)\n",
    "#dimとは→-2,0で列単位-1,1で行単位，なんで二個あるのかは知らん\n",
    "print(y_hat_1)\n",
    "Y_hat = torch.softmax(model(x_train[:4]), dim=-1)\n",
    "print(Y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 1.5027\n",
      "勾配:\n",
      "tensor([[-0.0007,  0.0051, -0.0050,  ..., -0.0023,  0.0006,  0.0050],\n",
      "        [-0.0116,  0.0850, -0.0835,  ..., -0.0382,  0.0093,  0.0838],\n",
      "        [ 0.0156, -0.1141,  0.1121,  ...,  0.0513, -0.0124, -0.1125],\n",
      "        [-0.0033,  0.0240, -0.0236,  ..., -0.0108,  0.0026,  0.0237]])\n"
     ]
    }
   ],
   "source": [
    "l_1 = criterion(model(x_train[:1]), y_train[:1])\n",
    "#入力ベクトルはsoftmax前の値\n",
    "#softmax後の値を入れたら，勾配が計算できなかった\n",
    "\n",
    "model.zero_grad()  # 勾配をゼロで初期化\n",
    "l_1.backward()  # 勾配を計算\n",
    "print(f'損失: {l_1:.4f}')\n",
    "print(f'勾配:\\n{model.weight.grad}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "  def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):  # len(dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "    return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_train = NewsDataset(x_train, y_train)\n",
    "dataset_valid = NewsDataset(x_valid, y_valid)\n",
    "dataset_test = NewsDataset(x_test, y_test)\n",
    "\n",
    "# Dataloaderの作成\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
    "#DataLoaderに必要なのは，Datasetオブジェクト，バッチサイズ，シャッフル\n",
    "#データセット：lenとgetitemを指定し，データへの一定のアクセス手法を提供する\n",
    "#バッチサイズ：？\n",
    "#シャッフル：エポックごとにデータをシャッフルするかどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of SLPNet(\n",
      "  (fc): Linear(in_features=300, out_features=4, bias=False)\n",
      ")>\n",
      "epoch: 1, loss_train: 0.5297, loss_valid: 0.3998\n",
      "epoch: 2, loss_train: 0.3648, loss_valid: 0.3648\n",
      "epoch: 3, loss_train: 0.3323, loss_valid: 0.3495\n",
      "epoch: 4, loss_train: 0.3155, loss_valid: 0.3437\n",
      "epoch: 5, loss_train: 0.3044, loss_valid: 0.3392\n",
      "epoch: 6, loss_train: 0.2970, loss_valid: 0.3303\n",
      "epoch: 7, loss_train: 0.2909, loss_valid: 0.3264\n",
      "epoch: 8, loss_train: 0.2868, loss_valid: 0.3243\n",
      "epoch: 9, loss_train: 0.2825, loss_valid: 0.3219\n",
      "epoch: 10, loss_train: 0.2799, loss_valid: 0.3234\n",
      "epoch: 11, loss_train: 0.2767, loss_valid: 0.3232\n",
      "epoch: 12, loss_train: 0.2748, loss_valid: 0.3198\n",
      "epoch: 13, loss_train: 0.2724, loss_valid: 0.3196\n",
      "epoch: 14, loss_train: 0.2711, loss_valid: 0.3221\n",
      "epoch: 15, loss_train: 0.2700, loss_valid: 0.3199\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model.parameters)\n",
    "\n",
    "#オプティマイザ：自動微分の結果を利用してモデルの更新を行うもの\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)#model.parametersとは\n",
    "\n",
    "# 学習\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "  # 訓練モードに設定\n",
    "  model.train()\n",
    "  loss_train = 0.0\n",
    "  for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "    # 勾配をゼロで初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 損失を記録\n",
    "    loss_train += loss.item()\n",
    " \n",
    "  # バッチ単位の平均損失計算\n",
    "  loss_train = loss_train / i\n",
    "\n",
    "  # 検証データの損失計算\n",
    "  model.eval() \n",
    "  with torch.no_grad():\n",
    "    inputs, labels = next(iter(dataloader_valid))\n",
    "    outputs = model(inputs)\n",
    "    loss_valid = criterion(outputs, labels)\n",
    "\n",
    "  # ログを出力\n",
    "  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率（学習データ）：0.908\n",
      "正解率（評価データ）：0.884\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, loader):\n",
    "  model.eval()\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in loader:\n",
    "      outputs = model(inputs)\n",
    "      pred = torch.argmax(outputs, dim=-1)\n",
    "      total += len(inputs)\n",
    "      correct += (pred == labels).sum().item()\n",
    "      \n",
    "  return correct / total\n",
    "\n",
    "acc_train = calculate_accuracy(model, dataloader_train)\n",
    "acc_test = calculate_accuracy(model, dataloader_test)\n",
    "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
    "print(f'正解率（評価データ）：{acc_test:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
